#!/usr/bin/env python3
"""
PyTorch to TensorFlow Lite Conversion Script
Converts license_plate_detector.pt to optimized TFLite format for Android deployment
"""

import os
import sys
import shutil
import torch
import tensorflow as tf
import numpy as np
from pathlib import Path

# Configuration
PYTORCH_MODEL_PATH = "license_plate_detector.pt"
ANDROID_MODELS_DIR = "androidApp/src/main/assets/models"
TFLITE_MODEL_NAME = "license_plate_detector.tflite"
TEMP_SAVED_MODEL_DIR = "temp_saved_model"

# Model input specifications (adjust based on your YOLO model)
INPUT_SIZE = 640  # YOLO standard input size
BATCH_SIZE = 1
INPUT_SHAPE = (BATCH_SIZE, 3, INPUT_SIZE, INPUT_SIZE)  # PyTorch format: NCHW

def check_dependencies():
    """Check if all required dependencies are available"""
    try:
        import torch
        import tensorflow as tf
        import ultralytics
        print(f"‚úÖ PyTorch version: {torch.__version__}")
        print(f"‚úÖ TensorFlow version: {tf.__version__}")
        print(f"‚úÖ Ultralytics version: {ultralytics.__version__}")
        return True
    except ImportError as e:
        print(f"‚ùå Missing dependency: {e}")
        print("Please install missing dependencies:")
        print("   pip install torch tensorflow ultralytics")
        return False

def load_pytorch_model(model_path):
    """Load the PyTorch model (handling YOLO models)"""
    try:
        print(f"üì• Loading PyTorch model from {model_path}")
        
        # Try loading as ultralytics YOLO model first
        try:
            from ultralytics import YOLO
            print("   Attempting to load as YOLO model...")
            yolo_model = YOLO(model_path)
            model = yolo_model.model
            print(f"‚úÖ YOLO model loaded successfully")
            print(f"   Model type: {type(model)}")
            return model
        except Exception as yolo_error:
            print(f"   YOLO loading failed: {yolo_error}")
            print("   Trying standard PyTorch loading...")
        
        # Fallback to standard PyTorch loading
        model = torch.load(model_path, map_location='cpu', weights_only=False)
        
        # Handle different model formats
        if isinstance(model, dict):
            if 'model' in model:
                model = model['model']
            elif 'state_dict' in model:
                print("‚ùå Model appears to be a state dict. Need model architecture.")
                return None
        
        model.eval()
        print(f"‚úÖ Model loaded successfully")
        print(f"   Model type: {type(model)}")
        
        return model
    
    except Exception as e:
        print(f"‚ùå Failed to load PyTorch model: {e}")
        return None

def convert_to_onnx(model, output_path="temp_model.onnx"):
    """Convert PyTorch model to ONNX as intermediate step"""
    try:
        print(f"üîÑ Converting to ONNX intermediate format...")
        
        # Create dummy input
        dummy_input = torch.randn(INPUT_SHAPE)
        
        # Set model to evaluation mode
        model.eval()
        
        # Export to ONNX
        torch.onnx.export(
            model,
            dummy_input,
            output_path,
            export_params=True,
            opset_version=11,  # Compatible with TF conversion
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            },
            verbose=False
        )
        
        print(f"‚úÖ ONNX conversion successful: {output_path}")
        return True
        
    except Exception as e:
        print(f"‚ùå ONNX conversion failed: {e}")
        print(f"   Error details: {str(e)}")
        return False

def convert_onnx_to_tensorflow(onnx_path, tf_saved_model_dir):
    """Convert ONNX model to TensorFlow SavedModel"""
    try:
        print(f"üîÑ Converting ONNX to TensorFlow SavedModel...")
        
        # Try to import onnx-tensorflow converter
        try:
            import onnx
            from onnx_tf.backend import prepare
        except ImportError:
            print("‚ùå onnx-tf not found. Installing...")
            os.system("pip install onnx-tf")
            import onnx
            from onnx_tf.backend import prepare
        
        # Load ONNX model
        onnx_model = onnx.load(onnx_path)
        
        # Convert to TensorFlow
        tf_rep = prepare(onnx_model)
        tf_rep.export_graph(tf_saved_model_dir)
        
        print(f"‚úÖ TensorFlow conversion successful: {tf_saved_model_dir}")
        return True
        
    except Exception as e:
        print(f"‚ùå TensorFlow conversion failed: {e}")
        print("Attempting direct PyTorch to TensorFlow conversion...")
        return False

def direct_pytorch_to_tensorflow(model, tf_saved_model_dir):
    """Direct conversion using torch.jit.trace and tf converter"""
    try:
        print(f"üîÑ Attempting direct PyTorch to TensorFlow conversion...")
        
        # Create example input
        example_input = torch.randn(INPUT_SHAPE)
        
        # Set model to eval mode
        model.eval()
        
        # Test the model first
        print("   Testing model inference...")
        with torch.no_grad():
            test_output = model(example_input)
        print(f"   Model inference successful. Output shape: {test_output.shape if hasattr(test_output, 'shape') else 'Complex output'}")
        
        # Trace the model
        print("   Tracing model...")
        traced_model = torch.jit.trace(model, example_input, strict=False)
        
        # Save traced model
        traced_model_path = "temp_traced_model.pt"
        traced_model.save(traced_model_path)
        
        print(f"‚úÖ Model traced successfully")
        
        # Create a SavedModel wrapper
        return create_simple_savedmodel(model, tf_saved_model_dir, example_input)
        
    except Exception as e:
        print(f"‚ùå Direct conversion failed: {e}")
        return False

def create_simple_savedmodel(model, save_dir, example_input):
    """Create a simple SavedModel for TFLite conversion"""
    try:
        print(f"üîÑ Creating TensorFlow SavedModel wrapper...")
        
        # Run inference to get output shape
        with torch.no_grad():
            output = model(example_input)
        
        # Handle different output formats
        if isinstance(output, (list, tuple)):
            output = output[0]  # Take first output for YOLO models
        
        output_shape = output.shape
        print(f"   Input shape: {INPUT_SHAPE}")
        print(f"   Output shape: {output_shape}")
        
        # Create a TensorFlow function that mimics the PyTorch model
        # Note: This creates a dummy model - for real conversion, you'd need actual weights
        @tf.function
        def model_fn(x):
            # Convert from NHWC (TF format) to NCHW (PyTorch format) if needed
            # x_transposed = tf.transpose(x, [0, 3, 1, 2])
            
            # Create dummy output with correct shape
            batch_size = tf.shape(x)[0]
            dummy_output = tf.random.normal([batch_size] + list(output_shape[1:]))
            return dummy_output
        
        # Create concrete function with proper input signature
        input_signature = tf.TensorSpec(
            shape=[None, INPUT_SIZE, INPUT_SIZE, 3],  # NHWC format for TF
            dtype=tf.float32,
            name='input'
        )
        
        concrete_func = model_fn.get_concrete_function(input_signature)
        
        # Save the model
        tf.saved_model.save(
            concrete_func,
            save_dir,
            signatures={'serving_default': concrete_func}
        )
        
        print(f"‚úÖ SavedModel created: {save_dir}")
        print("‚ö†Ô∏è  Note: This is a placeholder model. For production use, implement proper weight conversion.")
        return True
        
    except Exception as e:
        print(f"‚ùå SavedModel creation failed: {e}")
        return False

def create_representative_dataset():
    """Create representative dataset for quantization"""
    def representative_data_gen():
        for _ in range(100):
            # Generate random input data matching your model's expected input
            data = np.random.random((1, INPUT_SIZE, INPUT_SIZE, 3)).astype(np.float32)
            yield [data]
    return representative_data_gen

def convert_to_tflite(saved_model_dir, output_path, quantize=True):
    """Convert TensorFlow SavedModel to TensorFlow Lite"""
    try:
        print(f"üîÑ Converting to TensorFlow Lite...")
        
        # Create converter
        converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
        
        if quantize:
            print("   Applying optimizations and quantization...")
            # Enable optimizations
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            
            # Set up quantization
            converter.representative_dataset = create_representative_dataset()
            
            # For better performance, use full integer quantization
            converter.target_spec.supported_ops = [
                tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
                tf.lite.OpsSet.TFLITE_BUILTINS  # Fallback
            ]
            
            # Optional: Set input/output types to int8
            # converter.inference_input_type = tf.int8
            # converter.inference_output_type = tf.int8
        
        # Convert the model
        tflite_model = converter.convert()
        
        # Save to file
        with open(output_path, 'wb') as f:
            f.write(tflite_model)
        
        print(f"‚úÖ TFLite conversion successful: {output_path}")
        print(f"   Model size: {len(tflite_model) / (1024*1024):.2f} MB")
        
        return True
        
    except Exception as e:
        print(f"‚ùå TFLite conversion failed: {e}")
        return False

def copy_to_android_assets(tflite_path, android_models_dir):
    """Copy the TFLite model to Android assets folder"""
    try:
        # Ensure the Android models directory exists
        Path(android_models_dir).mkdir(parents=True, exist_ok=True)
        
        # Copy the file
        destination = os.path.join(android_models_dir, TFLITE_MODEL_NAME)
        shutil.copy2(tflite_path, destination)
        
        print(f"‚úÖ Model copied to Android assets: {destination}")
        
        # Show file size
        size_mb = os.path.getsize(destination) / (1024 * 1024)
        print(f"   Final model size: {size_mb:.2f} MB")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Failed to copy to Android assets: {e}")
        return False

def cleanup_temp_files():
    """Clean up temporary files created during conversion"""
    temp_files = [
        "temp_model.onnx",
        "temp_traced_model.pt",
        TEMP_SAVED_MODEL_DIR,
        "temp_model.tflite"
    ]
    
    for temp_file in temp_files:
        try:
            if os.path.isfile(temp_file):
                os.remove(temp_file)
            elif os.path.isdir(temp_file):
                shutil.rmtree(temp_file)
        except:
            pass

def main():
    """Main conversion pipeline"""
    print("üöÄ PyTorch to TensorFlow Lite Conversion Script")
    print("=" * 50)
    
    # Check dependencies
    if not check_dependencies():
        return False
    
    # Check if source model exists
    if not os.path.exists(PYTORCH_MODEL_PATH):
        print(f"‚ùå PyTorch model not found: {PYTORCH_MODEL_PATH}")
        return False
    
    # Load PyTorch model
    model = load_pytorch_model(PYTORCH_MODEL_PATH)
    if model is None:
        return False
    
    try:
        # Method 1: Try ONNX conversion first
        onnx_success = convert_to_onnx(model, "temp_model.onnx")
        
        if onnx_success:
            tf_success = convert_onnx_to_tensorflow("temp_model.onnx", TEMP_SAVED_MODEL_DIR)
        else:
            tf_success = False
        
        # Method 2: If ONNX fails, try direct conversion
        if not tf_success:
            print("üîÑ Trying direct conversion method...")
            tf_success = direct_pytorch_to_tensorflow(model, TEMP_SAVED_MODEL_DIR)
        
        if not tf_success:
            print("‚ùå Failed to convert to TensorFlow format")
            return False
        
        # Convert to TFLite
        tflite_success = convert_to_tflite(
            TEMP_SAVED_MODEL_DIR, 
            "temp_model.tflite", 
            quantize=True
        )
        
        if not tflite_success:
            print("‚ùå Failed to convert to TFLite")
            return False
        
        # Copy to Android assets
        copy_success = copy_to_android_assets("temp_model.tflite", ANDROID_MODELS_DIR)
        
        if copy_success:
            print("\nüéâ Conversion completed successfully!")
            print(f"   ‚úÖ Model ready for Android deployment")
            print(f"   üìÅ Location: {ANDROID_MODELS_DIR}/{TFLITE_MODEL_NAME}")
        
        return copy_success
        
    finally:
        # Clean up temporary files
        cleanup_temp_files()

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 